{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import imageio\n",
    "from openpyxl import load_workbook\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "\n",
    "def get_corpus(file):\n",
    "    # 导入title，得到title的list\n",
    "    wb = load_workbook(filename = file)  # Work Book\n",
    "    ws = wb.get_sheet_by_name('Sheet1')  # Work Sheet\n",
    "    column = ws['AA']  # Column\n",
    "    title = [column[x].value for x in range(1,len(column) -1)]\n",
    "    # print(len(column) -1)\n",
    "    # for i in range(len(column) - 2):\n",
    "    #     print(i)\n",
    "    # for each in title:\n",
    "    #     print(each)\n",
    "\n",
    "    # 导入content, 得到content的list\n",
    "    column1 = ws['AB']  # Column\n",
    "    content = [column1[x].value for x in range(1,len(column1)-1)]\n",
    "    # for i in range(0,10):\n",
    "    #     print(i)\n",
    "\n",
    "    # 合并title和content, 制作语料库\n",
    "    corpus = []\n",
    "    for i in range(len(column) - 2):\n",
    "        temp = ''\n",
    "        temp1 = ' '\n",
    "        temp += str(title[i])\n",
    "        temp1 += str(content[i])\n",
    "        temp2 = ''\n",
    "        temp2 += temp + temp1\n",
    "        corpus.append(temp2)\n",
    "    #print(corpus)\n",
    "    return corpus, ws, column1\n",
    "\n",
    "# 自制stopwords\n",
    "def get_stop_list(file):\n",
    "    stopwords_path = file\n",
    "    stop_list = []\n",
    "    with open(stopwords_path, \"r\", encoding=\"utf-8\") as f:\n",
    "         for line in f.readlines():\n",
    "             stop_list.append(line.replace(\"\\n\", \"\"))\n",
    "    return  stop_list\n",
    "\n",
    "def get_words_list(corpus, stop_list):\n",
    "    words_list = []\n",
    "    # 使用vectorizer的countVectorizer处理语料库，使用numpy转化成矩阵叫做count_matrix\n",
    "    vectorizer = CountVectorizer(stop_words=stop_list, ngram_range=(1,2), lowercase='True', max_df= 0.5, min_df = 10);\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    for i in vectorizer.get_feature_names():\n",
    "        words_list.append(i)\n",
    "\n",
    "    #print(words_list)\n",
    "    # print(len(vectorizer.get_feature_names_out()))\n",
    "    #print(X.toarray())\n",
    "    count_matrix = np.array(X.toarray())\n",
    "    # print('文章数量: ', count.shape[0])\n",
    "    # print('单词数量' , maxtrix.shape[1])\n",
    "    return words_list, count_matrix\n",
    "\n",
    "def get_category_column(ws, column1, category_name, category_number):\n",
    "    # 导入第一个类别表格，转换成列向量\n",
    "    column2 = ws[category_number]\n",
    "    category_name = [column2[x].value for x in range(1,len(column1)-1)]\n",
    "    category_name_temp = []\n",
    "    category_name_temp.append(category_name)\n",
    "    category_matrix = np.array(category_name_temp).T\n",
    "    #print(category_matrix)\n",
    "    category_m = (category_matrix - np.mean(category_matrix)) / np.std(category_matrix)\n",
    "    n = len(category_m)\n",
    "    sum1 = sum(float(category_m[i]) for i in range(n))\n",
    "    sum1_pow = sum([pow(v, 2.0) for v in category_m])\n",
    "    return sum1, sum1_pow, category_matrix, category_m\n",
    "\n",
    "def get_feature_words(count_matrix, category_m, sum1, sum1_pow, words_list, category_name):\n",
    "    category_pearson = {}\n",
    "    feature_words_list = []\n",
    "\n",
    "    n = len(category_m)\n",
    "    for c in range(count_matrix.shape[1]-1):\n",
    "        word = count_matrix[:, [c]]\n",
    "        #print(word)\n",
    "        sum2 = sum(float(word[i]) for i in range(n))\n",
    "        sum2_pow = sum([pow(v, 2.0) for v in word])\n",
    "        p_sum = sum([category_m[i] * word[i] for i in range(n)])\n",
    "        num = p_sum - (sum1 * sum2 / n)\n",
    "        den = math.sqrt((sum1_pow - pow(sum1, 2) / n) * (sum2_pow - pow(sum2, 2) / n))\n",
    "        if den == 0:\n",
    "            pearson = 0.0\n",
    "        else:\n",
    "            pearson = num / den\n",
    "            if pearson<0:\n",
    "                pearson = -pearson\n",
    "\n",
    "        #print(pearson)\n",
    "        category_pearson[words_list[c]] = pearson.tolist()\n",
    "        category_pearson_sort = dict(sorted(category_pearson.items(),key = operator.itemgetter(1), reverse=True))\n",
    "\n",
    "    for i in range(30):\n",
    "        keys_list = list(category_pearson_sort.keys())\n",
    "        feature_words_list.append(keys_list[i])\n",
    "    #print(feature_words)\n",
    "    return feature_words_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus, ws, column1 = get_corpus('D:/surf/tradition_medicine/Data/new_sheet.xlsx')\n",
    "    stop_list = get_stop_list(r\"D:/surf/tradition_medicine/Data/stop_words.txt\")\n",
    "    words_list, count_matrix = get_words_list(corpus, stop_list)\n",
    "    #print(words_list)\n",
    "    #print(words_list)\n",
    "\n",
    "    feature_words = {}\n",
    "    categories = {'humanint': 'T', 'responsi': 'U', 'morality': 'V', 'ecocons': 'W', 'conflict': 'X', 'leadersh': 'Y', 'factural': 'Z' }\n",
    "    save_dict = {'humanint': 'AC', 'responsi':'AD' , 'morality': 'AE', 'ecocons': 'AF', 'conflict':'AG', 'leadersh': 'AH', 'factural': 'AI'}\n",
    "    workbook = load_workbook('D:/surf/tradition_medicine/Data/new_sheet.xlsx')\n",
    "\n",
    "    sheet = workbook.active\n",
    "    for category in categories.keys():\n",
    "        sum1, sum1_pow, category_matrix, category_m = get_category_column(ws, column1, category, categories[category])\n",
    "        print(category_matrix)\n",
    "        feature_words_list = get_feature_words(count_matrix, category_m, sum1, sum1_pow, words_list, category)\n",
    "        feature_words[category] = feature_words_list\n",
    "\n",
    "    for key in save_dict.keys():\n",
    "        #print(key)\n",
    "        sheet[save_dict[key] + '1'] = key\n",
    "        for x in range(2, 12):\n",
    "            words = feature_words[key]\n",
    "            #print(words[x-2])\n",
    "            sheet[save_dict[key] + str(x)] = words[x-2]\n",
    "\n",
    "    print(feature_words)\n",
    "    workbook.save('D:/surf/tradition_medicine/Data/classified_sheet.xlsx')\n",
    "\n",
    "    ALL_FL_30= []\n",
    "    for value in feature_words.values():\n",
    "        feature_words_list = value\n",
    "        ALL_FL_30.append(feature_words_list)\n",
    "    i = 0\n",
    "    for document in ALL_FL_30:\n",
    "        i = i + 1\n",
    "        # 3).绘制词云\n",
    "        wc = WordCloud(\n",
    "            background_color=\"white\",  # 背景颜色\n",
    "            max_words=30,  # 显示最大词数\n",
    "            font_path=\"./image/simkai.ttf\",  # 使用字体\n",
    "            min_font_size=5,  # 最小字体\n",
    "            max_font_size=80,  # 最大字体\n",
    "            width=400,  # 图幅宽度\n",
    "            font_step=1\n",
    "        )\n",
    "        wc.generate(','.join(document))\n",
    "        wc.to_file('D:\\\\SURF\\\\tradition_medicine\\\\Data\\\\word_cloud\\\\%s.png' % (i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
